# whatscooking

This is my first kaggle challenge. My submission was in the top 25%. It ranked 329th out of 1388 teams. More information about the challenge, it's requirements and data can be found at https://www.kaggle.com/c/whats-cooking

While tackling this challenge, I tried to apply many of the things that I have learnt or re-learnt recently by taking the two online machine learning classes. It was quite fun and easy to be able to use many of the python based data analytic tools that I started using while I was still at Teza. 

Despite the ease of implementing some of the machine learning ideas, I realized that my computational resources were a critical bottle neck. Feature engineering requires you to try learning based on a wide number of permutations and combinations of different feature transformations. Even machine learning tools need you to do introspection by doing bias variance checking. All of this means that you are doing 1000s of "fits" to figure out the contours of your data, or "learn" about it and this requires you to be able to run many of these in parallel. Doing it on a dual core Macbook Air is not ideal. 

For my next challenge, I plan to set up a cloud computing network, perhaps through Amazon's EC2, where I can parallelize many of the computations and am, thus, able to learn faster.
